{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "print(device)\n",
    "block_size=8\n",
    "batch_size=4\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "#These are 2 hyperparameters that are really imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "with open('Wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(len(chars))\n",
    "print(chars)\n",
    "#an encoder will convert each element of this array into a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 54, 71, 62, 73, 68, 72, 61]\n",
      "paritosh\n"
     ]
    }
   ],
   "source": [
    "#Now we will map each character to a number so now any string we encode will be encoded in the above pattern of numbers\n",
    "\n",
    "string_to_int = { ch:i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s : [string_to_int[c] for c in s]\n",
    "decode = lambda l : ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "encoded_paritosh = encode(\"paritosh\")\n",
    "print(encoded_paritosh)\n",
    "decoded_paritosh = decode(encoded_paritosh)\n",
    "print(decoded_paritosh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26, 49,\n",
      "         0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,\n",
      "         0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47,\n",
      "        33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36,\n",
      "        25, 38, 28,  1, 39, 30,  1, 39, 50,  9])\n"
     ]
    }
   ],
   "source": [
    "### Listing some basic Torch Data Structures for reference:\n",
    "\n",
    " # Tensor: multi-dimensional array that can hold various types\n",
    " # Dataset: classes and its derivatives\n",
    " \n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split the data in validation and training splits\n",
    "\n",
    "Why can't we just use the entire data set and train on that?\n",
    "- So we basically have 18% train and 20% validation (e.g.) because we want the model to generate text that is ***like*** the corpus, not ***exactly***.\n",
    "- If we just train the model on the entire corpus then the model will just simple memorize all of it.\n",
    "\n",
    "Why is it a BIGRAM LANGUAGE MODEL\n",
    "\n",
    "- There are probabilities to predict what the next character in the word is based on the previoud one\n",
    "- e.g. if the word if \"Hello\", then\n",
    "  start of content -> H, H->e, e->l, l->l, l->o.\n",
    "- only gonna consider the previous chat to predict the next, therefore 2 = bi = bigram\n",
    "\n",
    "Let us begin by taking a block size = 5\n",
    "\n",
    "- ... [5, 67, 21, 58, 40] 35, .... (these are the encoded texts from corpus)\n",
    "- ... 5 [67, 21, 58, 40, 35], ....\n",
    "- these are prediction and targets, what the model would understand is that 67 is following 5\n",
    "- We look at how far is the prediction away from the target, then we can optimize for reducing that error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is  tensor([1]) , the target is  tensor(1)\n",
      "input is  tensor([1, 1]) , the target is  tensor(28)\n",
      "input is  tensor([ 1,  1, 28]) , the target is  tensor(39)\n",
      "input is  tensor([ 1,  1, 28, 39]) , the target is  tensor(42)\n",
      "input is  tensor([ 1,  1, 28, 39, 42]) , the target is  tensor(39)\n",
      "input is  tensor([ 1,  1, 28, 39, 42, 39]) , the target is  tensor(44)\n",
      "input is  tensor([ 1,  1, 28, 39, 42, 39, 44]) , the target is  tensor(32)\n",
      "input is  tensor([ 1,  1, 28, 39, 42, 39, 44, 32]) , the target is  tensor(49)\n"
     ]
    }
   ],
   "source": [
    "# block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(\"input is \", context, \", the target is \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically this is sequencial. We can't expect real time results if the processing is sequencial. Therefore we need batches of these blocks to process in parallel for any real time results from our bigram model.\n",
    "\n",
    "Hence, we need GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([162411, 118556,   7161,   1342])\n",
      "inputs :  tensor([[58, 71, 72,  1, 76, 68, 67,  5],\n",
      "        [74, 57, 57, 58, 71, 11,  1, 43],\n",
      "        [58, 58, 72,  1, 54,  1, 72, 61],\n",
      "        [74, 58, 72, 73, 72,  1, 59, 71]], device='cuda:0')\n",
      "targets :  tensor([[71, 72,  1, 76, 68, 67,  5, 73],\n",
      "        [57, 57, 58, 71, 11,  1, 43, 68],\n",
      "        [58, 72,  1, 54,  1, 72, 61, 68],\n",
      "        [58, 72, 73, 72,  1, 59, 71, 68]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) #pushing it to GPU\n",
    "    return x, y\n",
    "    \n",
    "x, y = get_batch('train')\n",
    "print(\"inputs : \", x)\n",
    "print(\"targets : \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram_LM(nn.Module):\n",
    "    #when we use the nn.module function, they are learnable parameters"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
