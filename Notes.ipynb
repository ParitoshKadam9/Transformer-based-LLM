{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "print(device)\n",
    "block_size=8\n",
    "batch_size=4\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "learning_rate = 3e-3\n",
    "eval_iters = 250\n",
    "drop_out = 0.2\n",
    "#These are 2 hyperparameters that are really imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "with open('Wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(len(chars))\n",
    "print(chars)\n",
    "vocab_size=len(chars)\n",
    "print(vocab_size)\n",
    "#an encoder will convert each element of this array into a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 54, 71, 62, 73, 68, 72, 61]\n",
      "paritosh\n"
     ]
    }
   ],
   "source": [
    "#Now we will map each character to a number so now any string we encode will be encoded in the above pattern of numbers\n",
    "\n",
    "string_to_int = { ch:i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s : [string_to_int[c] for c in s]\n",
    "decode = lambda l : ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "encoded_paritosh = encode(\"paritosh\")\n",
    "print(encoded_paritosh)\n",
    "decoded_paritosh = decode(encoded_paritosh)\n",
    "print(decoded_paritosh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26, 49,\n",
      "         0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,\n",
      "         0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47,\n",
      "        33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36,\n",
      "        25, 38, 28,  1, 39, 30,  1, 39, 50,  9])\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "### Listing some basic Torch Data Structures for reference:\n",
    "\n",
    " # Tensor: multi-dimensional array that can hold various types\n",
    " # Dataset: classes and its derivatives\n",
    " \n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232310\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split the data in validation and training splits\n",
    "\n",
    "Why can't we just use the entire data set and train on that?\n",
    "- So we basically have 18% train and 20% validation (e.g.) because we want the model to generate text that is ***like*** the corpus, not ***exactly***.\n",
    "- If we just train the model on the entire corpus then the model will just simple memorize all of it.\n",
    "\n",
    "Why is it a BIGRAM LANGUAGE MODEL\n",
    "\n",
    "- There are probabilities to predict what the next character in the word is based on the previoud one\n",
    "- e.g. if the word if \"Hello\", then\n",
    "  start of content -> H, H->e, e->l, l->l, l->o.\n",
    "- only gonna consider the previous chat to predict the next, therefore 2 = bi = bigram\n",
    "\n",
    "Let us begin by taking a block size = 5\n",
    "\n",
    "- ... [5, 67, 21, 58, 40] 35, .... (these are the encoded texts from corpus)\n",
    "- ... 5 [67, 21, 58, 40, 35], ....\n",
    "- these are prediction and targets, what the model would understand is that 67 is following 5\n",
    "- We look at how far is the prediction away from the target, then we can optimize for reducing that error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is  tensor([1]) , the target is  tensor(1)\n",
      "input is  tensor([1, 1]) , the target is  tensor(28)\n",
      "input is  tensor([ 1,  1, 28]) , the target is  tensor(39)\n",
      "input is  tensor([ 1,  1, 28, 39]) , the target is  tensor(42)\n",
      "input is  tensor([ 1,  1, 28, 39, 42]) , the target is  tensor(39)\n",
      "input is  tensor([ 1,  1, 28, 39, 42, 39]) , the target is  tensor(44)\n",
      "input is  tensor([ 1,  1, 28, 39, 42, 39, 44]) , the target is  tensor(32)\n",
      "input is  tensor([ 1,  1, 28, 39, 42, 39, 44, 32]) , the target is  tensor(49)\n"
     ]
    }
   ],
   "source": [
    "# block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(\"input is \", context, \", the target is \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically this is sequencial. We can't expect real time results if the processing is sequencial. Therefore we need batches of these blocks to process in parallel for any real time results from our bigram model.\n",
    "\n",
    "Hence, we need GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs :  tensor([[64, 62, 67, 60,  1, 54, 65, 65],\n",
      "        [58, 58, 71, 59, 74, 65, 65, 78],\n",
      "        [ 1, 54, 67, 57,  1, 61, 62, 72],\n",
      "        [73, 78,  1, 68, 59,  1, 73, 61]], device='cuda:0')\n",
      "targets :  tensor([[62, 67, 60,  1, 54, 65, 65,  1],\n",
      "        [58, 71, 59, 74, 65, 65, 78, 11],\n",
      "        [54, 67, 57,  1, 61, 62, 72,  1],\n",
      "        [78,  1, 68, 59,  1, 73, 61, 58]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) #pushing it to GPU\n",
    "    return x, y\n",
    "    \n",
    "x, y = get_batch('train')\n",
    "print(\"inputs : \", x)\n",
    "print(\"targets : \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Gradient Descent Work (recap)\n",
    "- Basically you take the ***derivative*** of the loss function which has the parameter we need to find. \n",
    "- If there are multiple parameters you need to find, you take ***derivative*** of the loss function with ***respect to*** all these parameters.\n",
    "- When you have 2 or more derivatives of the same function, they are called a **Gradient**.\n",
    "- We then use this ***Gradient*** to ***descend*** to the lowest point in the **Loss Function**\n",
    "- When the slope is far from 0, we must take longer steps since we are far from the optimal value, but as we're near 0, we need to take baby steps\n",
    "- step_size = slope*learning_rate ----- and therefore ---- new_intercept = old_intercept-step_size\n",
    "- we also need to limit it by the number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k]=loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X1Gs5\n",
      "JFrgyK.&58hKse9oL;!1JQOecQ]ip(u7n,GuX*cvy53L7)MLYt,;N\n",
      " wL7XbOyoV\".QJ;Cnu'1q;[C3ZN.i3-!qz7]h_g3dzj&:e)vcgUQR)f&HZdK)_bfzwNEl9oV_HL7uQuKD8NGzff'n7E6\".Rs*At(,G YUb(20mE_RIks9zhr5c\"V&PV3pqqHa;KWt.4rsTKGHLft]ua8wi:)P9F YE4RPmbg]X.RPecKRQX'.hQRQdxd8N- V!W1\n",
      "E6rd;(deuZzXu7)&Fy[GW0u-dCfs5JbW)CVb;MFlGtjUQRI3kaW6A?Q&g&YqvMC6Ey(sEXbwIM(fdMQegq7TqY*FHZL-B(0Mp_0.0\";ISF:o]wL7 myiyjxz2,GH4\n",
      "&RQZ'C\"Oecg648LkkKobYXBCvKtpAvbjtPD*yyYdFl'C,i\"2ldBBpY*FyWmyHZOm6qY4?QF7lVidiY!-u7uyyoHD0P\n",
      "m,4c'b7X*'-FokBCn8qvaksKTs\n"
     ]
    }
   ],
   "source": [
    "class Bigram_LM(nn.Module):\n",
    "    #when we use the nn.module function, they are learnable parameters\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #this is just a look-up table for the predictions of what comes after what probabilistically\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else :\n",
    "            B, T, C =logits.shape\n",
    "            logits = logits.view(B*T, C) # view just puts a tensor back together. a = a.view(x, y, z)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        # pretty much a probability distribution e.g. [0.167, 0.33, 0.5] are logits, they might represent ab, ac, ad. \n",
    "        # Logits are the raw, unnormalized predictions produced by a neural network's output layer before they are transformed into probabilities.\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] #becomes B, c , also, -1 will cause it to loop back to the last element\n",
    "            #apply softmax to get probabilities normalised\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #(B, 1)\n",
    "            #append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim = 1) # (B, T+1)\n",
    "        return index\n",
    "    \n",
    "model = Bigram_LM(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let me recap the above code cell :\n",
    "\n",
    "Let us Split the cell into 5 parts : \n",
    "### 1. __Initialization__ (__init__):\n",
    "#### a. Parameters \n",
    "- vocab_size = number of unique tokens the model can work with (chars & words)\n",
    "\n",
    "#### b. Embedding Table :\n",
    "- Initializes an embedding layer by using \"nn.Embedding\"\n",
    "- This layer is essentially a look-up table for word embeddings (probabilities)\n",
    "- It is used to transform discrete categorical data into continuous representations so that we can use this data for ML models to work with.\n",
    "- Why? In NLP words and tokens are typically represented as discrete values, like integers. e.g \"cat\" can be represented as 43.\n",
    "- Using these integers can be problematic, since they don't capture relationships and meanings between words, they treat words as independent and unrelated.\n",
    "- An Embedding table is basically a lookup table where each row corresponds to a unique token (words in a vocab) and each row contains a continuous vector rep (embedding) of that token.\n",
    "- Usage? During training of a neural network, you can use these embeddings as a part of the input data.\n",
    "- The network learns to adjust these embeddings during training to improve the performance of the task at hand.\n",
    "\n",
    "### 2. __Forward Pass (forward method)__ :\n",
    "#### a. Parameters\n",
    "- index = (tensor) represents a sequence of tokens\n",
    "- targets = (optional tensor) target tokens for training\n",
    "- They are used to compute the loss during training\n",
    "\n",
    "#### b. Logits\n",
    "- concept used especially used in classification and NNs.\n",
    "- They are raw, unnormalized scores or valies produced by last layer of NN before getting transformed into probabilities \n",
    "- They are a way to quantify how strongly the model believes a given input belongs to each class\n",
    "- In classification problems, logits are used to make decisions about which class an input most likely belongs to.\n",
    "- They provide a level of abstraction that allows the model to learn complex patterns in the data and assign confidence scores to different classes\n",
    "\n",
    "### 3. Generate method (text generation):\n",
    "#### a. Parameters \n",
    "- index = representing the unique tokens in our corpus\n",
    "- max_new_tokens = max num of tokens to generate\n",
    "\n",
    "#### b. function\n",
    "- inside the loop for **max_new_tokens** iterations, it repeatedly predicts the next token based on the previous tokens.\n",
    "- Each iteration uses ***forward*** method to obtain the logits for the next token\n",
    "- It selects the logits for the last time step (current prediction) and applies a softmax function to convert them into probabilities\n",
    "- It samples the next token index from the probability distribution using ***torch.multinomial***\n",
    "- The sample index is appended to the running sequence\n",
    "- ***This is how a text is generated***\n",
    "\n",
    "### 4. Model and Context : \n",
    "- Instance of Bigram_LM is created with a specific vocab_size and it is moved to the specified 'device'.\n",
    "- context is initialized as a tensor with zeros representing the starting point for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss {'train': tensor(2.4516), 'val': tensor(2.4752)}\n",
      "step: 1000, loss {'train': tensor(2.4113), 'val': tensor(2.4894)}\n",
      "step: 2000, loss {'train': tensor(2.4401), 'val': tensor(2.4814)}\n",
      "step: 3000, loss {'train': tensor(2.4505), 'val': tensor(2.4904)}\n",
      "step: 4000, loss {'train': tensor(2.4226), 'val': tensor(2.4731)}\n",
      "step: 5000, loss {'train': tensor(2.4045), 'val': tensor(2.4939)}\n",
      "step: 6000, loss {'train': tensor(2.4038), 'val': tensor(2.4870)}\n",
      "step: 7000, loss {'train': tensor(2.4256), 'val': tensor(2.4735)}\n",
      "step: 8000, loss {'train': tensor(2.4345), 'val': tensor(2.4642)}\n",
      "step: 9000, loss {'train': tensor(2.4276), 'val': tensor(2.4965)}\n",
      "2.252755880355835\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "max_iters =10000\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter%1000 ==0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step: {iter}, loss {losses}')\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What optimization? \n",
    "Appears to be a part of training loop for a machine learning model.\n",
    "\n",
    "### 1. __Optimizer Initialization__\n",
    "- initialize optimizer, specifically ***AdamW***\n",
    "- model.parameters() is passed which provides it with model's ***trainable parameters ***(weights, biases of nn)\n",
    "- ***lr = learning rate***, set to a specific value which controls the ***step size*** during the gradient descent\n",
    "\n",
    "### 2. __Training Loop :__\n",
    "- set max_iters to desired value\n",
    "- start a loop that will interate over the training data multiple times to train the model\n",
    "\n",
    "### 3. __Data Batch Retrieval (training data):__\n",
    "- xb, yb are the training indexes and tragets we need to train the data\n",
    "\n",
    "### 4. __Forward Pass and Loss Calculation :__\n",
    "- you call the foward method to calculate the models logits and losses\n",
    "- ***logits*** represent the models ***raw prediction scores***\n",
    "- ***loss*** represents the calculated loss, typically a measure of the difference bw the model's prediction and the the actual target.\n",
    "\n",
    "### 5. __Gradient Calculation & Backprop :__\n",
    "- before updating the model's parameters, you set the gradients of all model parameters to zero\n",
    "- Then you compute the gradients of the loss with respect to the model's parameters using ***loss.backward()***. This step is a part of the ***backpropagation*** algo and calculates how much each parametere should be adjusted to minimize loss.\n",
    "\n",
    "### 6. __Parameter update:__\n",
    "- optimizer.step()\n",
    "- finally you update the model's parameters based on the computed gradients and the learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\"I squin ncevetsthe\n",
      "Cand. thin and winef pore anver. skacel y  sant or tond sque upend-me shiro guesouroffe,  t,\"inowonseant aimowofred my\n",
      "\n",
      "iare itoum tofilin Whed tho mastof bos d\n",
      "\n",
      "ghilt'loonkoorod ndealll?\"Wind hor,\n",
      "\n",
      "  had T w Deall borang heryoplale g on\n",
      "Hebs aven'ltimy. r, d he b-thed the sed 95dofok, d witirlfathoury, han, don sha athavey The hemas\n",
      "othe wag  iza hesoma flle, ablld, t, heorgings bue\n",
      "tlcac. cedan too Whre \" a eth nd wecor--p, Dotshef GI cha plesof kysofus ag trew awhagourki\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "A Transformer has 4 basic functions : \n",
    "- Word Embedding\n",
    "- Positional Encoding\n",
    "- Self-Attention\n",
    "- Residual Connections\n",
    "\n",
    "--------------------------------------------------------------------------------------------------\n",
    "\n",
    "- Since a transformer is a type of NN, and NNs only have numbers for input values, hence we need to encode and decode the words\n",
    "- Therefore ***Word Embedding*** : use a relatively simple NN, that has 1 input for every word or symbol in the vocab that we want to use.\n",
    "- We call each input a ***Token***\n",
    "- The inputs are then connected to an **Activation Function**.\n",
    "- ![Alt text](image-1.png)\n",
    "- All of these *weights* are determined using ***BackPropagation***.\n",
    "- ***Positional Encoding*** : a technique that transformers use to keep track of word order.\n",
    "- We can use sin and cos waves to all these positional encodings\n",
    "- ![Alt text](image-2.png)\n",
    "- but, for e.g. \"The **Pizza** came out of the **oven** and **it** tasted good\", the transformers must associate the \"it\" with \"pizza\".\n",
    "- ***Self-Attention*** calcs similatiry of word and each other word\n",
    "- One way to calc ***similarity*** is called the **Dot Product**.\n",
    "- ![Alt text](image-3.png)\n",
    "\n",
    "- But we already had 2 values, why do we need more ? The new ***Self-Attention*** values for each word contain input from all the other words, and this helps give each word context.\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "- Instead of this process being sequential, the Transformer can do all of the computation for each word in the input at the same time\n",
    "- This is why we need a GPU\n",
    "- ![Alt text](image-4.png)\n",
    "----------------------------------------------\n",
    "\n",
    "Now that we are done with the encoder, we need to maintain a relative relation between the encoder and the decoder.\n",
    "- we find the similarities between each word in the encoder to each word in the decoder using ***dot product***\n",
    "- ![Alt text](image-5.png)\n",
    "- Then scale these values using the ***Softmax*** percentages.\n",
    "- Then add the pairs of scaled values together to get the ***Encoder-Decoder Attention*** values.\n",
    "- We then run these residual connection values through a ***Fully connected layer***\n",
    "- ![Alt text](image-6.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
